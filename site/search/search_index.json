{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"TrustLLM: Trustworthiness in Large Language Models","text":""},{"location":"index.html#about","title":"About","text":"<p>TrustLLM is a comprehensive study of trustworthiness in large language models (LLMs), including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. The document explains how to use the trustllm python package to help you assess the performance of your LLM in trustworthiness more quickly. For more details about TrustLLM, please refer to this link.</p> <p></p>"},{"location":"index.html#before-evaluation","title":"Before Evaluation","text":""},{"location":"index.html#installation","title":"Installation","text":"<p>Installation can be done using pypi:</p> <pre><code>pip install trustllm\n</code></pre>"},{"location":"index.html#dataset-download","title":"Dataset Download","text":"<p>Download TrustLLM dataset:</p> <pre><code>from trustllm import dataset_download\n\ndownload_huggingface_dataset(save_path='save_path')\n</code></pre>"},{"location":"index.html#generation","title":"Generation","text":"<p>Note</p> <p>Please note that the LLM you use for evaluation should have a certain level of utility. If its generation/NLP capabilities are weak, it may bias the evaluation results (for example, many evaluation samples may be considered invalid).</p> <p>The datasets are uniformly structured in JSON format, where each JSON file consists of a collection of dictionaries. Within each dictionary, there is a key named <code>prompt</code>. Your should utilize the value of <code>prompt</code> key as the input for generation. After generation, you should store the output of LLMs as s new key named <code>res</code> within the same dictionary. Here is an example to generate answer from your LLM:</p> <pre><code>import json\n\nfilename = 'dataset_path.json'\n\n# Load the data from the file\nwith open(filename, 'r') as file:\n    data = json.load(file)\n\n# Process each dictionary and add the 'res' key with the generated output\nfor element in data:\n    element['res'] = generation(element['prompt'])  # Replace 'generation' with your function\n\n# Write the modified data back to the file\nwith open(filename, 'w') as file:\n    json.dump(data, file, indent=4)\n</code></pre>"},{"location":"index.html#start-your-evaluation","title":"Start Your Evaluation","text":""},{"location":"index.html#api-setting","title":"API Setting","text":"<p>Before starting the evaluation, you need to first set up your OpenAI API (GPT-4-turbo) and Perspective API (used for measuring toxicity).</p> <pre><code>from trustllm import config\n\nconfig.openai_key = 'your-openai-api-key'\n\nconfig.perspective_key = 'your-perspective-api-key'\n</code></pre>"},{"location":"index.html#truthfulness","title":"Truthfulness","text":"<p>Four subsections in truthfulness evaluation:</p> <ul> <li>Misinformation: <code>external.json</code>, <code>internal.json</code></li> <li>Hallucination: <code>hallucination.json</code></li> <li>Sycophancy: <code>sycophancy.json</code></li> <li>Adversarial Factuality: <code>golden_advfactuality.json</code></li> </ul> <p>Requirement:</p> <p></p> <ul> <li>openai api (gpt-4-turbo)</li> </ul> <p>Preliminary:</p> <pre><code>from trustllm.task import truthfulness\nfrom trustllm.utils import file_process\nfrom trustllm import config\n\nevaluator = truthfulness.TruthfulnessEval()\n</code></pre> <p>Misinformation evaluation:</p> <pre><code>misinformation_internal_data = file_process.load_json('misinformation_internal_data_json_path')\nprint(evaluator.external_eval(misinformation_internal_data))\n\nmisinformation_external_data = file_process.load_json('misinformation_external_data_json_path')\nprint(evaluator.internal_eval(misinformation_external_data))\n</code></pre> <p>Hallucination evaluation:</p> <pre><code>hallucination_data = file_process.load_json('hallucination_data_json_path')\nprint(evaluator.hallucination_eval(hallucination_data))\n</code></pre> <p>Sycophancy evaluation (<code>eval_type</code>: type of evaluation, either <code>persona</code> or <code>preference</code>): <pre><code>sycophancy_data = file_process.load_json('sycophancy_data_json_path')\nprint(evaluator.sycophancy_eval(sycophancy_data, eval_type='persona'))\nprint(evaluator.sycophancy_eval(sycophancy_data, eval_type='preference'))\n</code></pre></p> <p>Adversarial factuality evaluation: <pre><code>adv_fact_data = file_process.load_json('adv_fact_data_json_path')\nprint(evaluator.advfact_eval(adv_fact_data))\n</code></pre></p>"},{"location":"index.html#safety","title":"Safety","text":"<p>Four subsections in safety evaluation:</p> <ul> <li>Jailbreak: <code>jailbreak.json</code></li> <li>Exaggerated Safety: <code>exaggerated_safety.json</code></li> <li>Toxicity: <code>jailbreak.json</code></li> <li>Misuse: <code>misuse.json</code></li> </ul> <p>Requirement:</p> <p> </p> <ul> <li>openai api (gpt-4-turbo)</li> <li>perspective api</li> <li>huggingface evaluator: LibrAI/longformer-harmful-ro</li> </ul> <p>Preliminary:</p> <pre><code>from trustllm.task import safety\nfrom trustllm.utils import file_process\nfrom trustllm import config\n\nevaluator = safety.SafetyEval()\n</code></pre> <p>Jailbreak evaluation (<code>eval_type</code>: type of evaluation, either <code>total</code> or <code>single</code>):</p> <pre><code>jailbreak_data = file_process.load_json('jailbreak_data_json_path')\nprint(evaluator.jailbreak_eval(jailbreak_data, eval_type='total')) # return overall RtA\nprint(evaluator.jailbreak_eval(jailbreak_data, eval_type='single')) # return RtA dict for each kind of jailbreak ways\n</code></pre> <p>Exaggerated evaluation:</p> <pre><code>exaggerated_data = file_process.load_json('exaggerated_data_json_path')\nprint(evaluator.exaggerated_eval(exaggerated_data))\n</code></pre> <p>Toxicity evaluation:</p> <p>Note</p> <p>Toxicity data for evaluation is based on the evaluated results of jailbreak data. You should first evaluate jailbreak data and then use evaluated results to conduct toxicity evaluation.</p> <pre><code># If you have not saved evaluated jailbreak results\njailbreak_data = file_process.load_json('jailbreak_data_json_path')\neval_data, _ = evaluator.jailbreak_eval(jailbreak_data, eval_type='total/single', return_dat=True)\nprint(evaluator.toxicity_eval(eval_data, perspective_api=config.perspective_key))\n\n# If you have saved evaluated jailbreak results\ntoxicity_data = file_process.load_json('evaluated_jailbreak_results_json_path') # load eval data for toxicity evaluation\nprint(evaluator.toxicity_eval(toxicity_data, perspective_api=config.perspective_key))\n</code></pre> <p>Misuse evaluation:</p> <pre><code>misuse_data = file_process.load_json('misuse_data.json')\nprint(evaluator.misuse_eval(misuse_data))\n</code></pre>"},{"location":"index.html#fairness","title":"Fairness","text":"<p>Three subsections in fairness evaluation:</p> <ul> <li>Stereotype: <code>stereotype_agreement.json</code>, <code>stereotype_recognition.json</code>, <code>stereotype_query_test.json</code></li> <li>Disparagement: <code>disparagement.json</code></li> <li>Preference: <code>preference_force.json</code></li> </ul> <p>Requirement:</p> <p> </p> <ul> <li>openai api (gpt-4-turbo)</li> <li>huggingface evaluator: LibrAI/longformer-harmful-ro</li> </ul> <p>Preliminary:</p> <pre><code>from trustllm.task import fairness\nfrom trustllm.utils import file_process\nfrom trustllm import config\n\nevaluator = fairness.FairnessEval()\n</code></pre> <p>Stereotype evaluation:</p> <pre><code>stereotype_recognition_data = file_process.load_json('stereotype_recognition_data_json_path')\nprint(evaluator.stereotype_recognition_eval(stereotype_recognition_data))\n\nstereotype_agreement_data = file_process.load_json('stereotype_agreement_data_json_path')\nprint(evaluator.stereotype_agreement_eval(stereotype_agreement_data))\n\nstereotype_query_test_data = file_process.load_json('stereotype_query_test_data_json_path')\nprint(evaluator.stereotype_query_eval(stereotype_query_test_data))\n</code></pre> <p>Disparagement evaluation: <pre><code>disparagement_data = file_process.load_json('disparagement_data_json_path')\nprint(evaluator.disparagement_eval(disparagement_data))\n</code></pre></p> <p>Preference evaluation: <pre><code>preference_data = file_process.load_json('preference_data_json_path')\nprint(evaluator.preference_eval(preference_data, type='plain'))\nprint(evaluator.preference_eval(preference_data, type='force'))\n</code></pre></p>"},{"location":"index.html#robustness","title":"Robustness","text":"<p>Two subsections in robustness evaluation:</p> <ul> <li>Natural noise: <code>advglue.json</code>, <code>advinstruction.json</code></li> <li>Out of distribution: <code>ood_generalization.json</code>, <code>ood_detection.json</code></li> </ul> <p>Requirement:</p> <p> </p> <ul> <li>huggingface evaluator: LibrAI/longformer-harmful-ro</li> </ul> <p>Preliminary:</p> <pre><code>from trustllm.task import robustness\nfrom trustllm.utils import file_process\nfrom trustllm import config\n\nevaluator = robustness.RobustnessEval()\n</code></pre> <p>Natural noise evaluation:</p> <pre><code>advglue_data = file_process.load_json('advglue_data_json_path')\nprint(evaluator.advglue_eval(advglue_data))\n\nadvinstruction_data = file_process.load_json('advinstruction_data_json_path')\nprint(evaluator.advglue_eval(advinstruction_data))\n</code></pre> <p>OOD evaluation:</p> <pre><code>ood_detection_data = file_process.load_json('ood_detection_data_json_path')\nprint(evaluator.ood_detection(ood_detection_data))\n\nood_generalization_data = file_process.load_json('ood_generalization_data_json_path')\nprint(evaluator.ood_generalization(ood_generalization_data))\n</code></pre>"},{"location":"index.html#privacy","title":"Privacy","text":"<p>Two subsections in privacy evaluation:</p> <ul> <li>Privacy awareness: <code>privacy_awareness_confAIde.json</code>, <code>privacy_awareness_query.json</code></li> <li>Privacy leakage: <code>privacy_leakage.json</code></li> </ul> <p>Requirement:</p> <p> </p> <ul> <li>huggingface evaluator: LibrAI/longformer-harmful-ro</li> </ul> <p>Preliminary:</p> <pre><code>from trustllm.task import privacy\nfrom trustllm.utils import file_process\nfrom trustllm import config\n\nevaluator = privacy.PrivacyEval()\n</code></pre> <p>Privacy awareness:</p> <pre><code>privacy_confAIde_data = file_process.load_json('privacy_confAIde_data_json_path')\nprint(evaluator.ConfAIDe_eval(privacy_confAIde_data))\n\nprivacy_awareness_query_data = file_process.load_json('privacy_awareness_query_data_json_path')\nprint(evaluator.awareness_query_eval(privacy_awareness_query_data, type='normal'))\nprint(evaluator.awareness_query_eval(privacy_awareness_query_data, type='aug'))\n</code></pre> <p>Privacy leakage:</p> <pre><code>privacy_leakage_data = file_process.load_json('privacy_leakage_data_json_path')\nprint(evaluator.leakage_eval(privacy_leakage_data))\n</code></pre>"},{"location":"index.html#machine-ethics","title":"Machine Ethics","text":"<p>Three subsections in machine ethics evaluation:</p> <p>Implicit ethics: <code>implicit_ETHICS.json</code>, <code>implicit_SocialChemistry101.json</code> Explicit ethics: <code>explicit_moralchoice.json</code> Emotional awareness: <code>emotional_awareness.json</code></p> <p>Requirement:</p> <p> </p> <ul> <li>openai api (gpt-4-turbo)</li> <li>huggingface evaluator: LibrAI/longformer-harmful-ro</li> </ul> <p>Preliminary:</p> <pre><code>from trustllm.task import ethics\nfrom trustllm.utils import file_process\nfrom trustllm import config\n\nevaluator = ethics.EthicsEval()\n</code></pre> <p>Explicit ethics:</p> <p><pre><code>explicit_ethics_data = file_process.load_json('explicit_ethics_data_json_path')\nprint(evaluator.explicit_ethics_eval(explicit_ethics_data, eval_type='low'))\nprint(evaluator.explicit_ethics_eval(explicit_ethics_data, eval_type='high'))\n</code></pre> Implicit ethics:</p> <pre><code>implicit_ethics_data = file_process.load_json('implicit_ethics_data_json_path')\nprint(evaluator.implicit_ethics_eval(implicit_ethics_data, eval_type='ETHICS'))\nprint(evaluator.implicit_ethics_eval(implicit_ethics_data, eval_type='STEREOTYPE'))\n</code></pre> <p>Emotional awareness:</p> <pre><code>emotional_awareness_data = file_process.load_json('emotional_awareness_data_json_path')\nprint(evaluator.emotional_awareness_eval(emotional_awareness_data))\n</code></pre>"},{"location":"index.html#leaderboard","title":"Leaderboard","text":"<p>If you want to view the performance of all models or upload the performance of your LLM, please refer to this link.</p>"},{"location":"index.html#citation","title":"Citation","text":""},{"location":"changelog.html","title":"Changelog","text":""},{"location":"changelog.html#version-010","title":"Version 0.1.0","text":""}]}